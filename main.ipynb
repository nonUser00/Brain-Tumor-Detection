{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["n4GEh0wyVTy-","RyejcyJpUnJA","dXRUWw7VV-ky","G9o-L2VSXTSF","ZoGyaotIXXta","gTukFKoXXY2X","4XeaHOgWXaLx","dhIv8wUjXapx","O9qg9p6EXbSA","QpyYU97xXb52"],"gpuType":"T4","authorship_tag":"ABX9TyP+HF+BKqSlohHQxNkl/4gW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# INSTALL LIBRARY"],"metadata":{"id":"n4GEh0wyVTy-"}},{"cell_type":"code","source":["!pip install python-dotenv xgboost scikit-learn opencv-python-headless -q"],"metadata":{"id":"4A-D09GcVQpr","executionInfo":{"status":"ok","timestamp":1763038930358,"user_tz":-420,"elapsed":3888,"user":{"displayName":"YUKA WARDANA","userId":"05679451618255931072"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# 1. MOUNT GOOGLE DRIVE"],"metadata":{"id":"RyejcyJpUnJA"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wAPFeGT-sXlS","executionInfo":{"status":"ok","timestamp":1762874206847,"user_tz":-420,"elapsed":21701,"user":{"displayName":"YUKA WARDANA","userId":"05679451618255931072"}},"outputId":"7d2112a4-9a66-4048-dbab-f8542f5175f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","PROJECT_DIR = '/content/drive/MyDrive/Python/Brain-Tumor-Detection'\n","os.makedirs(PROJECT_DIR, exist_ok=True)\n","os.chdir(PROJECT_DIR)"],"metadata":{"id":"AMGd6nsvUj9w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. IMPORT LIBRARY"],"metadata":{"id":"dXRUWw7VV-ky"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import cv2\n","from dotenv import load_dotenv\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from xgboost import XGBClassifier\n","from tensorflow.keras.applications import VGG16, InceptionV3, ResNet101, DenseNet201\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","import zipfile\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"Kpq12JwnWVjG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. LOAD FILE .env"],"metadata":{"id":"G9o-L2VSXTSF"}},{"cell_type":"code","source":["env_path = os.path.join(PROJECT_DIR, '.env')\n","load_dotenv(env_path)\n","kaggle_username = os.getenv('KAGGLE_USERNAME')\n","kaggle_key = os.getenv('KAGGLE_KEY')"],"metadata":{"id":"O027mzP2XuL2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. KONFIGURASI KAGGLE API"],"metadata":{"id":"ZoGyaotIXXta"}},{"cell_type":"code","source":["os.makedirs('/root/.kaggle', exist_ok=True)\n","import json\n","kaggle_json = {\n","    \"username\": kaggle_username,\n","    \"key\": kaggle_key\n","}\n","\n","kaggle_json_path = '/root/.kaggle/kaggle.json'\n","with open(kaggle_json_path, 'w') as f:\n","    json.dump(kaggle_json, f)\n","\n","os.chmod(kaggle_json_path, 0o600)"],"metadata":{"id":"4HzSlcpyYV_5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. BUAT STRUKTUR FOLDER PROJECT"],"metadata":{"id":"aZVwlmrIXYcT"}},{"cell_type":"code","source":["DATASETS_DIR = os.path.join(PROJECT_DIR, 'datasets')\n","RESULTS_DIR = os.path.join(PROJECT_DIR, 'results')\n","MODELS_DIR = os.path.join(PROJECT_DIR, 'models')\n","\n","os.makedirs(DATASETS_DIR, exist_ok=True)\n","os.makedirs(RESULTS_DIR, exist_ok=True)\n","os.makedirs(MODELS_DIR, exist_ok=True)\n","\n","print(f\"Struktur folder:\")\n","print(f\"- Datasets: {DATASETS_DIR}\")\n","print(f\"- Results:  {RESULTS_DIR}\")\n","print(f\"- Models:   {MODELS_DIR}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plvOURZbY7k5","executionInfo":{"status":"ok","timestamp":1762874219222,"user_tz":-420,"elapsed":16,"user":{"displayName":"YUKA WARDANA","userId":"05679451618255931072"}},"outputId":"e48e5e81-6cf8-47ca-a9aa-d278ce0da4f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Struktur folder:\n","- Datasets: /content/drive/MyDrive/Python/Brain-Tumor-Detection/datasets\n","- Results:  /content/drive/MyDrive/Python/Brain-Tumor-Detection/results\n","- Models:   /content/drive/MyDrive/Python/Brain-Tumor-Detection/models\n"]}]},{"cell_type":"markdown","source":["# 6. DOWNLOAD DATASETS"],"metadata":{"id":"gTukFKoXXY2X"}},{"cell_type":"code","source":["def download_datasets():\n","    dataset1_zip = os.path.join(DATASETS_DIR, 'brain-mri-images-for-brain-tumor-detection.zip')\n","    dataset2_zip = os.path.join(DATASETS_DIR, 'brain-tumor-detection.zip')\n","\n","    dataset1_dir = os.path.join(DATASETS_DIR, 'dataset1')\n","    dataset2_dir = os.path.join(DATASETS_DIR, 'dataset2')\n","\n","    if not os.path.exists(dataset1_dir):\n","        print(\"Download Dataset 1: Navoneel Brain Tumor...\")\n","        os.system(f'kaggle datasets download -d navoneel/brain-mri-images-for-brain-tumor-detection -p {DATASETS_DIR}')\n","\n","        with zipfile.ZipFile(dataset1_zip, 'r') as zip_ref:\n","            zip_ref.extractall(dataset1_dir)\n","        print(\"Dataset 1 berhasil didownload dan diekstrak\")\n","    else:\n","        print(\"Dataset 1 sudah ada\")\n","\n","    if not os.path.exists(dataset2_dir):\n","        print(\"Download Dataset 2: Br35H Brain Tumor...\")\n","        os.system(f'kaggle datasets download -d ahmedhamada0/brain-tumor-detection -p {DATASETS_DIR}')\n","\n","        with zipfile.ZipFile(dataset2_zip, 'r') as zip_ref:\n","            zip_ref.extractall(dataset2_dir)\n","        print(\"Dataset 2 berhasil didownload dan diekstrak\")\n","    else:\n","        print(\"Dataset 2 sudah ada\")\n","\n","download_datasets()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ivMtivckZWjr","executionInfo":{"status":"ok","timestamp":1762874219228,"user_tz":-420,"elapsed":5,"user":{"displayName":"YUKA WARDANA","userId":"05679451618255931072"}},"outputId":"a9644858-8cd6-4cd9-8df8-83a6997bc4a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset 1 sudah ada\n","Dataset 2 sudah ada\n"]}]},{"cell_type":"markdown","source":["# 7. FUNCTION PREPROCESSING"],"metadata":{"id":"4XeaHOgWXaLx"}},{"cell_type":"code","source":["def crop_brain_region(image):\n","    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n","    _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n","\n","    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    if contours:\n","        largest_contour = max(contours, key=cv2.contourArea)\n","        x, y, w, h = cv2.boundingRect(largest_contour)\n","        cropped = image[y:y+h, x:x+w]\n","        return cropped\n","    return image\n","\n","def apply_median_filter(image):\n","    return cv2.medianBlur(image, 5)\n","\n","def preprocess_image(img_path, target_size=(224, 224), apply_filter=True):\n","    try:\n","        img = cv2.imread(img_path)\n","        if img is None:\n","            return None\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","        img = crop_brain_region(img)\n","\n","        img = cv2.resize(img, target_size)\n","\n","        if apply_filter:\n","            img = apply_median_filter(img)\n","\n","        return img\n","    except Exception as e:\n","        print(f\"Error processing {img_path}: {e}\")\n","        return None"],"metadata":{"id":"Lym0h-WFayo7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 8. DATA AUGMENTATION"],"metadata":{"id":"dhIv8wUjXapx"}},{"cell_type":"code","source":["def create_augmented_data(images, labels, augmentation_factor=3):\n","  datagen = ImageDataGenerator(\n","        horizontal_flip=True,\n","        vertical_flip=True,\n","        rotation_range=20,\n","        width_shift_range=0.1,\n","        height_shift_range=0.1\n","    )\n","\n","  augmented_images = []\n","  augmented_labels = []\n","\n","  for img, label in zip(images, labels):\n","        augmented_images.append(img)\n","        augmented_labels.append(label)\n","\n","        img_reshaped = img.reshape((1,) + img.shape)\n","\n","        count = 0\n","        for batch in datagen.flow(img_reshaped, batch_size=1):\n","            augmented_images.append(batch[0])\n","            augmented_labels.append(label)\n","            count += 1\n","            if count >= augmentation_factor:\n","                break\n","\n","  return np.array(augmented_images), np.array(augmented_labels)\n","\n","def load_dataset(dataset_path, dataset_type='dataset1'):\n","    images = []\n","    labels = []\n","\n","    if dataset_type == 'dataset1':\n","        yes_path = os.path.join(dataset_path, 'yes')\n","        no_path = os.path.join(dataset_path, 'no')\n","\n","        if os.path.exists(yes_path):\n","            for img_file in os.listdir(yes_path):\n","                if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n","                    img_path = os.path.join(yes_path, img_file)\n","                    img = preprocess_image(img_path, target_size=(224, 224))\n","                    if img is not None:\n","                        images.append(img)\n","                        labels.append(1)\n","\n","        if os.path.exists(no_path):\n","            for img_file in os.listdir(no_path):\n","                if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n","                    img_path = os.path.join(no_path, img_file)\n","                    img = preprocess_image(img_path, target_size=(224, 224))\n","                    if img is not None:\n","                        images.append(img)\n","                        labels.append(0)\n","\n","    elif dataset_type == 'dataset2':\n","        yes_path = os.path.join(dataset_path, 'yes')\n","        no_path = os.path.join(dataset_path, 'no')\n","\n","        if os.path.exists(yes_path):\n","            yes_files = [f for f in os.listdir(yes_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n","            for img_file in yes_files[:1500]:\n","                img_path = os.path.join(yes_path, img_file)\n","                img = preprocess_image(img_path, target_size=(224, 224))\n","                if img is not None:\n","                    images.append(img)\n","                    labels.append(1)\n","\n","        if os.path.exists(no_path):\n","            no_files = [f for f in os.listdir(no_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n","            for img_file in no_files[:1500]:\n","                img_path = os.path.join(no_path, img_file)\n","                img = preprocess_image(img_path, target_size=(224, 224))\n","                if img is not None:\n","                    images.append(img)\n","                    labels.append(0)\n","\n","    return np.array(images), np.array(labels)\n","\n","def extract_features_vgg16(images):\n","    base_model = VGG16(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n","    features = base_model.predict(images, batch_size=32, verbose=0)\n","    return features\n","\n","def extract_features_inceptionv3(images):\n","    images_resized = np.array([cv2.resize(img, (299, 299)) for img in images])\n","    base_model = InceptionV3(weights='imagenet', include_top=False, pooling='avg', input_shape=(299, 299, 3))\n","    features = base_model.predict(images_resized, batch_size=32, verbose=0)\n","    return features\n","\n","def extract_features_resnet101(images):\n","    base_model = ResNet101(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n","    features = base_model.predict(images, batch_size=32, verbose=0)\n","    return features\n","\n","def extract_features_densenet201(images):\n","    base_model = DenseNet201(weights='imagenet', include_top=False, pooling='avg', input_shape=(224, 224, 3))\n","    features = base_model.predict(images, batch_size=32, verbose=0)\n","    return features\n","\n","def extract_all_features(images):\n","    feat_vgg = extract_features_vgg16(images)\n","    feat_inc = extract_features_inceptionv3(images)\n","    feat_res = extract_features_resnet101(images)\n","    feat_den = extract_features_densenet201(images)\n","\n","    return feat_vgg, feat_inc, feat_res, feat_den\n"],"metadata":{"id":"aPE7Aw1-b8WA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 9. PENGGABUNGAN FEATURE"],"metadata":{"id":"O9qg9p6EXbSA"}},{"cell_type":"code","source":["def concatenate_features(feat_vgg, feat_inc, feat_res, feat_den=None):\n","    if feat_den is None:\n","        concatenated = np.concatenate([feat_vgg, feat_inc, feat_res], axis=1)\n","    else:\n","        concatenated = np.concatenate([feat_vgg, feat_inc, feat_res, feat_den], axis=1)\n","\n","    return concatenated"],"metadata":{"id":"SNO6QnyUc3XX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 10. GENETIC ALGORITHM FEATURE SELECTION"],"metadata":{"id":"QpyYU97xXb52"}},{"cell_type":"code","source":["def genetic_feature_selection(X_train, y_train, n_features=500, population_size=50, generations=20):\n","    n_total_features = X_train.shape[1]\n","\n","    def fitness_function(chromosome):\n","        selected_features = np.where(chromosome == 1)[0]\n","        if len(selected_features) == 0:\n","            return 0\n","\n","        X_subset = X_train[:, selected_features]\n","        fitness = 0\n","\n","        for j in range(X_subset.shape[0]):\n","            for k in range(X_subset.shape[1]):\n","                val = X_subset[j, k]\n","                fitness += np.log(val + 1e-8) * val\n","\n","        fitness = fitness / X_subset.shape[0]\n","        return fitness\n","\n","    population = []\n","    for _ in range(population_size):\n","        chromosome = np.zeros(n_total_features, dtype=int)\n","        selected_idx = np.random.choice(n_total_features, n_features, replace=False)\n","        chromosome[selected_idx] = 1\n","        population.append(chromosome)\n","\n","    for generation in range(generations):\n","        fitness_scores = [fitness_function(chromo) for chromo in population]\n","\n","        sorted_indices = np.argsort(fitness_scores)[::-1]\n","        population = [population[i] for i in sorted_indices]\n","\n","        elite_size = int(0.1 * population_size)\n","        new_population = population[:elite_size]\n","\n","        while len(new_population) < population_size:\n","            parent1 = population[np.random.randint(0, population_size // 2)]\n","            parent2 = population[np.random.randint(0, population_size // 2)]\n","\n","            crossover_point = np.random.randint(1, n_total_features)\n","            child = np.concatenate([parent1[:crossover_point], parent2[crossover_point:]])\n","\n","            mutation_rate = 0.01\n","            for i in range(len(child)):\n","                if np.random.random() < mutation_rate:\n","                    child[i] = 1 - child[i]\n","\n","            new_population.append(child)\n","\n","        population = new_population\n","\n","    best_chromosome = population[0]\n","    selected_indices = np.where(best_chromosome == 1)[0]\n","\n","    if len(selected_indices) > n_features:\n","        selected_indices = selected_indices[:n_features]\n","    elif len(selected_indices) < n_features:\n","        remaining = n_features - len(selected_indices)\n","        unselected = np.where(best_chromosome == 0)[0]\n","        additional = np.random.choice(unselected, remaining, replace=False)\n","        selected_indices = np.concatenate([selected_indices, additional])\n","\n","    print(f\"{len(selected_indices)} features terpilih dari {n_total_features} features\")\n","    return selected_indices"],"metadata":{"id":"Cc7cBj_zdLUv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 11. TRAINING DAN EVALUASI"],"metadata":{"id":"PKrCb9qQXc3u"}},{"cell_type":"code","source":["def train_and_evaluate(X_train, X_test, y_train, y_test, classifier_name='SVM'):\n","    if classifier_name == 'SVM':\n","        classifier = SVC(kernel='rbf', C=10.0, gamma='scale', random_state=42)\n","    elif classifier_name == 'RandomForest':\n","        classifier = RandomForestClassifier(\n","            n_estimators=200,\n","            max_depth=20,\n","            min_samples_split=2,\n","            min_samples_leaf=1,\n","            random_state=42\n","        )\n","    elif classifier_name == 'DecisionTree':\n","        classifier = DecisionTreeClassifier(\n","            max_depth=20,\n","            min_samples_split=2,\n","            min_samples_leaf=1,\n","            random_state=42\n","        )\n","    elif classifier_name == 'XGBoost':\n","        classifier = XGBClassifier(\n","            n_estimators=200,\n","            learning_rate=0.1,\n","            max_depth=8,\n","            subsample=0.8,\n","            colsample_bytree=0.8,\n","            random_state=42,\n","            use_label_encoder=False,\n","            eval_metric='logloss'\n","        )\n","\n","    print(f\"Training {classifier_name}...\")\n","    classifier.fit(X_train, y_train)\n","\n","    y_pred = classifier.predict(X_test)\n","\n","    accuracy = accuracy_score(y_test, y_pred)\n","    precision = precision_score(y_test, y_pred, average='binary')\n","    recall = recall_score(y_test, y_pred, average='binary')\n","    f1 = f1_score(y_test, y_pred, average='binary')\n","\n","    results = {\n","        'Classifier': classifier_name,\n","        'Accuracy': accuracy,\n","        'Precision': precision,\n","        'Recall': recall,\n","        'F1-Score': f1\n","    }\n","\n","    import pickle\n","    model_path = os.path.join(MODELS_DIR, f'{classifier_name}_model.pkl')\n","    with open(model_path, 'wb') as f:\n","        pickle.dump(classifier, f)\n","    print(f\"Model disimpan di: {model_path}\")\n","\n","    return results, classifier\n"],"metadata":{"id":"wOjesagAdjTz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 12. MAIN EXPERIMENT"],"metadata":{"id":"7yqmks--duSk"}},{"cell_type":"code","source":["def run_experiment(dataset_choice='dataset1'):\n","    print(f\"{'='*60}\")\n","    print(f\"EKSPERIMEN DATASET: {dataset_choice.upper()}\")\n","    print(f\"{'='*60}\")\n","    if dataset_choice == 'dataset1':\n","        dataset_dir = os.path.join(DATASETS_DIR, 'dataset1')\n","        images, labels = load_dataset(dataset_dir, 'dataset1')\n","    else:\n","        dataset_dir = os.path.join(DATASETS_DIR, 'dataset2')\n","        images, labels = load_dataset(dataset_dir, 'dataset2')\n","\n","    print(f\"\\nTotal Dataset: {len(images)} images\")\n","    print(f\"- Tumor: {np.sum(labels == 1)}\")\n","    print(f\"- Non-tumor: {np.sum(labels == 0)}\")\n","\n","    print(\"\\nMelakukan augmentasi data...\")\n","    images_aug, labels_aug = create_augmented_data(images, labels, augmentation_factor=3)\n","    print(f\"Augmented dataset: {len(images_aug)} images\")\n","\n","    images_aug = images_aug.astype('float32') / 255.0\n","\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        images_aug, labels_aug, test_size=0.2, random_state=42, stratify=labels_aug\n","    )\n","\n","    print(f\"\\nData split:\")\n","    print(f\"- Training: {len(X_train)} images\")\n","    print(f\"- Testing: {len(X_test)} images\")\n","\n","    print(\"\\nEkstraksi Deep Features...\")\n","    feat_vgg_train, feat_inc_train, feat_res_train, feat_den_train = extract_all_features(X_train)\n","    feat_vgg_test, feat_inc_test, feat_res_test, feat_den_test = extract_all_features(X_test)\n","\n","    print(\"\\nPenggabungan features dari 3 model (VGG-16, Inception V3, ResNet-101)...\")\n","    X_train_concat = concatenate_features(feat_vgg_train, feat_inc_train, feat_res_train)\n","    X_test_concat = concatenate_features(feat_vgg_test, feat_inc_test, feat_res_test)\n","\n","    print(f\"Concatenated features shape: {X_train_concat.shape}\")\n","\n","    selected_features = genetic_feature_selection(X_train_concat, y_train, n_features=500)\n","    X_train_selected = X_train_concat[:, selected_features]\n","    X_test_selected = X_test_concat[:, selected_features]\n","\n","    print(f\"\\nSelected features shape: {X_train_selected.shape}\")\n","    features_path = os.path.join(RESULTS_DIR, f'selected_features_{dataset_choice}.npy')\n","    np.save(features_path, selected_features)\n","\n","    print(\"\\nTraining dan Evaluasi Model...\")\n","    classifiers = ['SVM', 'RandomForest', 'DecisionTree', 'XGBoost']\n","    all_results = []\n","    for clf_name in classifiers:\n","        results, model = train_and_evaluate(X_train_selected, X_test_selected, y_train, y_test, clf_name)\n","        all_results.append(results)\n","        print(f\"{clf_name} Results:\")\n","        print(f\"Accuracy:  {results['Accuracy']:.4f} ({results['Accuracy']*100:.2f}%)\")\n","        print(f\"Precision: {results['Precision']:.4f}\")\n","        print(f\"Recall:    {results['Recall']:.4f}\")\n","        print(f\"F1-Score:  {results['F1-Score']:.4f}\\n\")\n","\n","    results_df = pd.DataFrame(all_results)\n","    print(f\"\\n{'='*60}\")\n","    print(\"HASIL PERFORMA SETIAP MODEL\")\n","    print(f\"{'='*60}\")\n","    print(results_df.round(4).to_string(index=False))\n","\n","    best_result = results_df.loc[results_df['Accuracy'].idxmax()]\n","    print(f\"\\nModel Terbaik: {best_result['Classifier']} - dengan Accuracy {best_result['Accuracy']:.4f} ({best_result['Accuracy']*100:.2f}%)\")\n","\n","\n","    results_csv = os.path.join(RESULTS_DIR, f'results_{dataset_choice}.csv')\n","    results_df.to_csv(results_csv, index=False)\n","    print(f\"\\nHasil Training dan Evaluasi Model disimpan di: {results_csv}\")\n","\n","    return results_df"],"metadata":{"id":"r-BLqxK9d_2A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 13. RUN EKSPERIMEN"],"metadata":{"id":"6fatNNY7ewGZ"}},{"cell_type":"code","source":["print(\"\\n\" + \"=\"*60)\n","print(\"REPLIKASI JURNAL - BRAIN TUMOR DETECTION\")\n","print(\"=\"*60)\n","\n","results_dataset1 = run_experiment('dataset1')\n","results_dataset2 = run_experiment('dataset2')\n","\n","print(\"\\nEKSPERIMEN SELESAI!\")\n","print(f\"\\nhasil tersimpan di:\")\n","print(f\"- {RESULTS_DIR}\")\n","print(f\"- {MODELS_DIR}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NW9o5y1Ve0qJ","outputId":"5a94008e-13d4-41ac-9bfc-6ec0702aaac2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","============================================================\n","REPLIKASI JURNAL - BRAIN TUMOR DETECTION\n","============================================================\n","\n","Memulai Eksperimen Dataset 1...\n","============================================================\n","EKSPERIMEN DATASET: DATASET1\n","============================================================\n","\n","\n","Loading dataset...\n","\n","Dataset loaded: 253 images\n","- Tumor: 155\n","- Non-tumor: 98\n","\n","Melakukan augmentasi data...\n","Augmented dataset: 1012 images\n","\n","Data split:\n","- Training: 809 images\n","- Testing: 203 images\n","\n","Ekstraksi Deep Features...\n","Mengekstrak features dari VGG-16...\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Mengekstrak features dari Inception V3...\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Mengekstrak features dari ResNet-101...\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m171446536/171446536\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n","Mengekstrak features dari DenseNet-201...\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\u001b[1m74836368/74836368\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Mengekstrak features dari VGG-16...\n","Mengekstrak features dari Inception V3...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 34 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79ea01ba2700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["Mengekstrak features dari ResNet-101...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 15 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79e9aa5bb100> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["Mengekstrak features dari DenseNet-201...\n","\n","Konkatenasi features dari 3 model (VGG-16, Inception V3, ResNet-101)...\n","Concatenated features shape: (809, 4608)\n","500 features terpilih dari 4608 features\n","\n","Selected features shape: (809, 500)\n","Selected features indices disimpan di: /content/drive/MyDrive/Python/Brain-Tumor-Detection/results/selected_features_dataset1.npy\n","\n","Training dan Evaluasi Model...\n","Training SVM...\n","Model disimpan di: /content/drive/MyDrive/Python/Brain-Tumor-Detection/models/SVM_model.pkl\n","\n","SVM Results:\n","Accuracy:  0.8966 (89.66%)\n","Precision: 0.9055\n","Recall:    0.9274\n","F1-Score:  0.9163\n","Training RandomForest...\n","Model disimpan di: /content/drive/MyDrive/Python/Brain-Tumor-Detection/models/RandomForest_model.pkl\n","\n","RandomForest Results:\n","Accuracy:  0.8916 (89.16%)\n","Precision: 0.8864\n","Recall:    0.9435\n","F1-Score:  0.9141\n","Training DecisionTree...\n","Model disimpan di: /content/drive/MyDrive/Python/Brain-Tumor-Detection/models/DecisionTree_model.pkl\n","\n","DecisionTree Results:\n","Accuracy:  0.8177 (81.77%)\n","Precision: 0.8321\n","Recall:    0.8790\n","F1-Score:  0.8549\n","Training XGBoost...\n","Model disimpan di: /content/drive/MyDrive/Python/Brain-Tumor-Detection/models/XGBoost_model.pkl\n","\n","XGBoost Results:\n","Accuracy:  0.9113 (91.13%)\n","Precision: 0.9015\n","Recall:    0.9597\n","F1-Score:  0.9297\n","\n","============================================================\n","RINGKASAN HASIL\n","============================================================\n","  Classifier  Accuracy  Precision   Recall  F1-Score\n","         SVM  0.896552   0.905512 0.927419  0.916335\n","RandomForest  0.891626   0.886364 0.943548  0.914062\n","DecisionTree  0.817734   0.832061 0.879032  0.854902\n","     XGBoost  0.911330   0.901515 0.959677  0.929688\n","\n","BEST MODEL: XGBoost\n","   Accuracy: 0.9113 (91.13%)\n","\n","Hasil disimpan di: /content/drive/MyDrive/Python/Brain-Tumor-Detection/results/results_dataset1.csv\n","\n","Memulai Eksperimen Dataset 2...\n","============================================================\n","EKSPERIMEN DATASET: DATASET2\n","============================================================\n","\n","\n","Loading dataset...\n","\n","Dataset loaded: 3000 images\n","- Tumor: 1500\n","- Non-tumor: 1500\n","\n","Melakukan augmentasi data...\n"]}]}]}